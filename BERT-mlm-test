# code adpated from the provided base file in https://docs.pytorch.org/TensorRT/_notebooks/Hugging-Face-BERT.html

from transformers import BertTokenizer, BertForMaskedLM, pipeline
import torch
import timeit
import numpy as np
#import torch_tensorrt
#
import torch.backends.cudnn as cudnn

# Initialize the tokenizer from the model
enc = BertTokenizer.from_pretrained('bert-base-uncased')
# Initalize pipeline to view prediction steps
fill_mask = pipeline("fill-mask", model="bert-base-uncased")

# Dummy inputs to generate a traced TorchScript model
batch_size = 4

batched_indexed_tokens = [[101, 64]*64]*batch_size
batched_segment_ids = [[0, 1]*64]*batch_size
batched_attention_masks = [[1, 1]*64]*batch_size

tokens_tensor = torch.tensor(batched_indexed_tokens)
segments_tensor = torch.tensor(batched_segment_ids)
attention_masks_tensor = torch.tensor(batched_attention_masks)

# Get BERT masked language model from Hugging Face
# Use dummy inputs to trace it
mlm_model_ts = BertForMaskedLM.from_pretrained('bert-base-uncased', torchscript=True)
traced_mlm_model = torch.jit.trace(mlm_model_ts, [tokens_tensor, segments_tensor, attention_masks_tensor])

# Define masked sentences
masked_sentences = ['He kicked a soccer [MASK] and it broke the window.']


#Tokenize Sentences
print('Original', masked_sentences[0])
print('Tokenized:', enc.tokenize(masked_sentences[0]))
print('Token IDs:', enc.convert_tokens_to_ids(enc.tokenize(masked_sentences[0])))


# Get predictions
predictions = fill_mask(masked_sentences[0])
# Display results
for pred in predictions:
    print(f"Token: {pred['token_str']}, Score: {pred['score']:.4f}")


# Pass masked sentences into MLM model -- incld. padding as sentences are different lengths
encoded_inputs = enc(masked_sentences, return_tensors='pt', padding='max_length', max_length=128)
outputs = mlm_model_ts(**encoded_inputs)
#outputs = traced_mlm_model(encoded_inputs['input_ids'], encoded_inputs['token_type_ids'], encoded_inputs['attention_mask'])

# Generate unmasked sentences + verify
# Dynamically find [MASK] token positions
input_ids = encoded_inputs["input_ids"]
mask_token_id = enc.mask_token_id  # This is 103 for BERT
pos_masks = [torch.where(seq == mask_token_id)[0].item() for seq in input_ids]


# Get most likely token IDs at each masked position
most_likely_token_ids = [torch.argmax(outputs[0][i, pos, :]).item() for i, pos in enumerate(pos_masks)]
# Decode each token individually to avoid space/subword issues
unmasked_tokens = [enc.decode([token_id]).strip() for token_id in most_likely_token_ids]
# Replace [MASK] with decoded token
unmasked_sentences = [masked_sentences[i].replace('[MASK]', token) for i, token in enumerate(unmasked_tokens)]


for sentence in unmasked_sentences:
    print(sentence)

